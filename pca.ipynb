{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba4734388a52bf2d",
   "metadata": {},
   "source": [
    "# Applying normal Timewise PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81de4f71f8e8d55d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:11:34.358649Z",
     "start_time": "2025-09-02T16:11:34.357026Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import title\n",
    "\n",
    "sns.set_theme(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf7e20c0b6e6d9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:11:34.368618Z",
     "start_time": "2025-09-02T16:11:34.365989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load segmented data\n",
    "def load_segmented_data(data_folder=\"\", event_name=None):\n",
    "    \"\"\"\n",
    "    Load segmented CSV data.\n",
    "\n",
    "    Parameters:\n",
    "    - data_folder (str): Folder where the segmented .csv files are stored.\n",
    "    - event_name (str or None): Name of the event to load. If None, loads all CSV files.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Loaded dataframe with all data concatenated if loading all events.\n",
    "    \"\"\"\n",
    "    if event_name is not None:\n",
    "        # Load one specific event csv\n",
    "        file_path = os.path.join(data_folder, f\"segment_around_{event_name}.csv\")\n",
    "        if not os.path.isfile(file_path):\n",
    "            raise FileNotFoundError(f\"No data found for event '{event_name}' at {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded data for event '{event_name}' from {file_path}.\")\n",
    "        return df\n",
    "    else:\n",
    "        # Load all csv files in the folder that match pattern 'segment_around_*.csv'\n",
    "        all_files = [f for f in os.listdir(data_folder) if f.startswith(\"segment_around_\") and f.endswith(\".csv\")]\n",
    "        if not all_files:\n",
    "            raise FileNotFoundError(f\"No segmented CSV files found in folder {data_folder}\")\n",
    "\n",
    "        dfs = []\n",
    "        for file in all_files:\n",
    "            path = os.path.join(data_folder, file)\n",
    "            df_temp = pd.read_csv(path)\n",
    "            # Optionally add a column indicating source event, parsed from filename\n",
    "            event_from_file = file.replace(\"segment_around_\", \"\").replace(\".csv\", \"\")\n",
    "            df_temp['EventName'] = event_from_file\n",
    "            dfs.append(df_temp)\n",
    "        df_all = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"Loaded and concatenated {len(all_files)} segmented event files from {data_folder}.\")\n",
    "        return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "351074c5b4779c24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:11:34.966436Z",
     "start_time": "2025-09-02T16:11:34.374670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data for event 'StagEventNew_nearest_neighbor_steeringRemoved' from data/cleaned_data/data_segment/Safak/segment_around_StagEventNew_nearest_neighbor_steeringRemoved.csv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_REF</th>\n",
       "      <th>uid</th>\n",
       "      <th>dataset</th>\n",
       "      <th>city_section</th>\n",
       "      <th>ExperimentalCondition</th>\n",
       "      <th>UnixTimeStamp</th>\n",
       "      <th>TobiiTimeStamp</th>\n",
       "      <th>RightEyeIsBlinkingWorld</th>\n",
       "      <th>RightEyeIsBlinkingLocal</th>\n",
       "      <th>LeftEyeIsBlinkingWorld</th>\n",
       "      <th>...</th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>TimeStamp_in_sec</th>\n",
       "      <th>TimeDiff_in_sec</th>\n",
       "      <th>Blink</th>\n",
       "      <th>BlinkDuration</th>\n",
       "      <th>BlinkDurationsLong</th>\n",
       "      <th>BlinkAfter</th>\n",
       "      <th>SamplingRate</th>\n",
       "      <th>time_from_event</th>\n",
       "      <th>outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-26 13:46:22.260000+00:00</td>\n",
       "      <td>0037b2329de444c18d751b4e79901b39</td>\n",
       "      <td>EyeTracking</td>\n",
       "      <td>MountainRoad</td>\n",
       "      <td>BaseCondition</td>\n",
       "      <td>1.601128e+09</td>\n",
       "      <td>347.918274</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1.601128e+09</td>\n",
       "      <td>139.759740</td>\n",
       "      <td>0.010971</td>\n",
       "      <td>False</td>\n",
       "      <td>0.106051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-5.00</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-26 13:46:22.280000+00:00</td>\n",
       "      <td>0037b2329de444c18d751b4e79901b39</td>\n",
       "      <td>EyeTracking</td>\n",
       "      <td>MountainRoad</td>\n",
       "      <td>BaseCondition</td>\n",
       "      <td>1.601128e+09</td>\n",
       "      <td>347.939484</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1.601128e+09</td>\n",
       "      <td>139.781682</td>\n",
       "      <td>0.021942</td>\n",
       "      <td>False</td>\n",
       "      <td>0.105959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-4.98</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-26 13:46:22.300000+00:00</td>\n",
       "      <td>0037b2329de444c18d751b4e79901b39</td>\n",
       "      <td>EyeTracking</td>\n",
       "      <td>MountainRoad</td>\n",
       "      <td>BaseCondition</td>\n",
       "      <td>1.601128e+09</td>\n",
       "      <td>347.961914</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1.601128e+09</td>\n",
       "      <td>139.803248</td>\n",
       "      <td>0.021566</td>\n",
       "      <td>False</td>\n",
       "      <td>0.105866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-4.96</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-26 13:46:22.320000+00:00</td>\n",
       "      <td>0037b2329de444c18d751b4e79901b39</td>\n",
       "      <td>EyeTracking</td>\n",
       "      <td>MountainRoad</td>\n",
       "      <td>BaseCondition</td>\n",
       "      <td>1.601128e+09</td>\n",
       "      <td>347.983795</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1.601128e+09</td>\n",
       "      <td>139.825190</td>\n",
       "      <td>0.016456</td>\n",
       "      <td>False</td>\n",
       "      <td>0.105774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-4.94</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-26 13:46:22.340000+00:00</td>\n",
       "      <td>0037b2329de444c18d751b4e79901b39</td>\n",
       "      <td>EyeTracking</td>\n",
       "      <td>MountainRoad</td>\n",
       "      <td>BaseCondition</td>\n",
       "      <td>1.601128e+09</td>\n",
       "      <td>348.000427</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1.601128e+09</td>\n",
       "      <td>139.841646</td>\n",
       "      <td>0.016705</td>\n",
       "      <td>False</td>\n",
       "      <td>0.105681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-4.92</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80155</th>\n",
       "      <td>2020-09-19 10:22:15.060000+00:00</td>\n",
       "      <td>ff4288f304e74bbf93aa6508c7df8145</td>\n",
       "      <td>EyeTracking</td>\n",
       "      <td>MountainRoad</td>\n",
       "      <td>FullLoopAR</td>\n",
       "      <td>1.600511e+09</td>\n",
       "      <td>717.885681</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1.600511e+09</td>\n",
       "      <td>170.287970</td>\n",
       "      <td>0.009973</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.92</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80156</th>\n",
       "      <td>2020-09-19 10:22:15.080000+00:00</td>\n",
       "      <td>ff4288f304e74bbf93aa6508c7df8145</td>\n",
       "      <td>EyeTracking</td>\n",
       "      <td>MountainRoad</td>\n",
       "      <td>FullLoopAR</td>\n",
       "      <td>1.600511e+09</td>\n",
       "      <td>717.907898</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1.600511e+09</td>\n",
       "      <td>170.310909</td>\n",
       "      <td>0.022938</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.94</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80157</th>\n",
       "      <td>2020-09-19 10:22:15.100000+00:00</td>\n",
       "      <td>ff4288f304e74bbf93aa6508c7df8145</td>\n",
       "      <td>EyeTracking</td>\n",
       "      <td>MountainRoad</td>\n",
       "      <td>FullLoopAR</td>\n",
       "      <td>1.600511e+09</td>\n",
       "      <td>717.919006</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1.600511e+09</td>\n",
       "      <td>170.322876</td>\n",
       "      <td>0.010971</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.96</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80158</th>\n",
       "      <td>2020-09-19 10:22:15.120000+00:00</td>\n",
       "      <td>ff4288f304e74bbf93aa6508c7df8145</td>\n",
       "      <td>EyeTracking</td>\n",
       "      <td>MountainRoad</td>\n",
       "      <td>FullLoopAR</td>\n",
       "      <td>1.600511e+09</td>\n",
       "      <td>717.952576</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1.600511e+09</td>\n",
       "      <td>170.355788</td>\n",
       "      <td>0.022938</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.98</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80159</th>\n",
       "      <td>2020-09-19 10:22:15.140000+00:00</td>\n",
       "      <td>ff4288f304e74bbf93aa6508c7df8145</td>\n",
       "      <td>EyeTracking</td>\n",
       "      <td>MountainRoad</td>\n",
       "      <td>FullLoopAR</td>\n",
       "      <td>1.600511e+09</td>\n",
       "      <td>717.974609</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1.600511e+09</td>\n",
       "      <td>170.377729</td>\n",
       "      <td>0.021942</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80160 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          timestamp_REF                               uid  \\\n",
       "0      2020-09-26 13:46:22.260000+00:00  0037b2329de444c18d751b4e79901b39   \n",
       "1      2020-09-26 13:46:22.280000+00:00  0037b2329de444c18d751b4e79901b39   \n",
       "2      2020-09-26 13:46:22.300000+00:00  0037b2329de444c18d751b4e79901b39   \n",
       "3      2020-09-26 13:46:22.320000+00:00  0037b2329de444c18d751b4e79901b39   \n",
       "4      2020-09-26 13:46:22.340000+00:00  0037b2329de444c18d751b4e79901b39   \n",
       "...                                 ...                               ...   \n",
       "80155  2020-09-19 10:22:15.060000+00:00  ff4288f304e74bbf93aa6508c7df8145   \n",
       "80156  2020-09-19 10:22:15.080000+00:00  ff4288f304e74bbf93aa6508c7df8145   \n",
       "80157  2020-09-19 10:22:15.100000+00:00  ff4288f304e74bbf93aa6508c7df8145   \n",
       "80158  2020-09-19 10:22:15.120000+00:00  ff4288f304e74bbf93aa6508c7df8145   \n",
       "80159  2020-09-19 10:22:15.140000+00:00  ff4288f304e74bbf93aa6508c7df8145   \n",
       "\n",
       "           dataset  city_section ExperimentalCondition  UnixTimeStamp  \\\n",
       "0      EyeTracking  MountainRoad         BaseCondition   1.601128e+09   \n",
       "1      EyeTracking  MountainRoad         BaseCondition   1.601128e+09   \n",
       "2      EyeTracking  MountainRoad         BaseCondition   1.601128e+09   \n",
       "3      EyeTracking  MountainRoad         BaseCondition   1.601128e+09   \n",
       "4      EyeTracking  MountainRoad         BaseCondition   1.601128e+09   \n",
       "...            ...           ...                   ...            ...   \n",
       "80155  EyeTracking  MountainRoad            FullLoopAR   1.600511e+09   \n",
       "80156  EyeTracking  MountainRoad            FullLoopAR   1.600511e+09   \n",
       "80157  EyeTracking  MountainRoad            FullLoopAR   1.600511e+09   \n",
       "80158  EyeTracking  MountainRoad            FullLoopAR   1.600511e+09   \n",
       "80159  EyeTracking  MountainRoad            FullLoopAR   1.600511e+09   \n",
       "\n",
       "       TobiiTimeStamp  RightEyeIsBlinkingWorld  RightEyeIsBlinkingLocal  \\\n",
       "0          347.918274                    False                    False   \n",
       "1          347.939484                    False                    False   \n",
       "2          347.961914                    False                    False   \n",
       "3          347.983795                    False                    False   \n",
       "4          348.000427                    False                    False   \n",
       "...               ...                      ...                      ...   \n",
       "80155      717.885681                    False                    False   \n",
       "80156      717.907898                    False                    False   \n",
       "80157      717.919006                    False                    False   \n",
       "80158      717.952576                    False                    False   \n",
       "80159      717.974609                    False                    False   \n",
       "\n",
       "       LeftEyeIsBlinkingWorld  ...     TimeStamp  TimeStamp_in_sec  \\\n",
       "0                       False  ...  1.601128e+09        139.759740   \n",
       "1                       False  ...  1.601128e+09        139.781682   \n",
       "2                       False  ...  1.601128e+09        139.803248   \n",
       "3                       False  ...  1.601128e+09        139.825190   \n",
       "4                       False  ...  1.601128e+09        139.841646   \n",
       "...                       ...  ...           ...               ...   \n",
       "80155                    True  ...  1.600511e+09        170.287970   \n",
       "80156                    True  ...  1.600511e+09        170.310909   \n",
       "80157                    True  ...  1.600511e+09        170.322876   \n",
       "80158                    True  ...  1.600511e+09        170.355788   \n",
       "80159                    True  ...  1.600511e+09        170.377729   \n",
       "\n",
       "       TimeDiff_in_sec  Blink  BlinkDuration  BlinkDurationsLong  BlinkAfter  \\\n",
       "0             0.010971  False       0.106051                 0.0       False   \n",
       "1             0.021942  False       0.105959                 0.0       False   \n",
       "2             0.021566  False       0.105866                 0.0       False   \n",
       "3             0.016456  False       0.105774                 0.0       False   \n",
       "4             0.016705  False       0.105681                 0.0       False   \n",
       "...                ...    ...            ...                 ...         ...   \n",
       "80155         0.009973  False            NaN                 0.0       False   \n",
       "80156         0.022938  False            NaN                 0.0       False   \n",
       "80157         0.010971  False            NaN                 0.0       False   \n",
       "80158         0.022938  False            NaN                 0.0       False   \n",
       "80159         0.021942  False            NaN                 0.0       False   \n",
       "\n",
       "       SamplingRate  time_from_event  outlier  \n",
       "0              50.0            -5.00    False  \n",
       "1              50.0            -4.98    False  \n",
       "2              50.0            -4.96    False  \n",
       "3              50.0            -4.94    False  \n",
       "4              50.0            -4.92    False  \n",
       "...             ...              ...      ...  \n",
       "80155          50.0             4.92    False  \n",
       "80156          50.0             4.94    False  \n",
       "80157          50.0             4.96    False  \n",
       "80158          50.0             4.98     True  \n",
       "80159          50.0             5.00     True  \n",
       "\n",
       "[80160 rows x 88 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data path\n",
    "data_folder = \"data/cleaned_data/data_segment/Safak\"\n",
    "# Load single event:\n",
    "df_single_event = load_segmented_data(data_folder=data_folder, event_name=\"StagEventNew_nearest_neighbor_steeringRemoved\")\n",
    "df_single_event\n",
    "# Load all events:\n",
    "# df_all = load_segmented_data(data_folder=\"data/data_segment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e8a5d181b1d391b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:11:35.009597Z",
     "start_time": "2025-09-02T16:11:35.005021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_single_event['uid'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4eff364d8c51b57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:11:35.031135Z",
     "start_time": "2025-09-02T16:11:35.029052Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp_REF', 'uid', 'dataset', 'city_section',\n",
       "       'ExperimentalCondition', 'UnixTimeStamp', 'TobiiTimeStamp',\n",
       "       'RightEyeIsBlinkingWorld', 'RightEyeIsBlinkingLocal',\n",
       "       'LeftEyeIsBlinkingWorld', 'LeftEyeIsBlinkingLocal', 'HmdPosition.x',\n",
       "       'HmdPosition.y', 'HmdPosition.z', 'NoseVector.x', 'NoseVector.y',\n",
       "       'NoseVector.z', 'EyePosWorldCombined.x', 'EyePosWorldCombined.y',\n",
       "       'EyePosWorldCombined.z', 'EyeDirWorldCombined.x',\n",
       "       'EyeDirWorldCombined.y', 'EyeDirWorldCombined.z',\n",
       "       'EyePosLocalCombined.x', 'EyePosLocalCombined.y',\n",
       "       'EyePosLocalCombined.z', 'EyeDirLocalCombined.x',\n",
       "       'EyeDirLocalCombined.y', 'EyeDirLocalCombined.z', 'ObjectName_1',\n",
       "       'ObjectName_2', 'ObjectName_3', 'ObjectName_4', 'ObjectName_5',\n",
       "       'HitObjectPosition.x_1', 'HitObjectPosition.x_2',\n",
       "       'HitObjectPosition.x_3', 'HitObjectPosition.x_4',\n",
       "       'HitObjectPosition.x_5', 'HitObjectPosition.y_1',\n",
       "       'HitObjectPosition.y_2', 'HitObjectPosition.y_3',\n",
       "       'HitObjectPosition.y_4', 'HitObjectPosition.y_5',\n",
       "       'HitObjectPosition.z_1', 'HitObjectPosition.z_2',\n",
       "       'HitObjectPosition.z_3', 'HitObjectPosition.z_4',\n",
       "       'HitObjectPosition.z_5', 'HitPointOnObject.x_1', 'HitPointOnObject.x_2',\n",
       "       'HitPointOnObject.x_3', 'HitPointOnObject.x_4', 'HitPointOnObject.x_5',\n",
       "       'HitPointOnObject.y_1', 'HitPointOnObject.y_2', 'HitPointOnObject.y_3',\n",
       "       'HitPointOnObject.y_4', 'HitPointOnObject.y_5', 'HitPointOnObject.z_1',\n",
       "       'HitPointOnObject.z_2', 'HitPointOnObject.z_3', 'HitPointOnObject.z_4',\n",
       "       'HitPointOnObject.z_5', 'distanceToPlayer_1', 'distanceToPlayer_2',\n",
       "       'distanceToPlayer_3', 'distanceToPlayer_4', 'distanceToPlayer_5',\n",
       "       'EventName', 'StartofEventTimeStamp', 'EndOfEventTimeStamp',\n",
       "       'EventDuration', 'SuccessfulCompletionState', 'SteeringInput',\n",
       "       'AcellerationInput', 'BrakeInput', 'ReceivedInput', 'TimeStamp',\n",
       "       'TimeStamp_in_sec', 'TimeDiff_in_sec', 'Blink', 'BlinkDuration',\n",
       "       'BlinkDurationsLong', 'BlinkAfter', 'SamplingRate', 'time_from_event',\n",
       "       'outlier'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_single_event.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479ac888ca155d17",
   "metadata": {},
   "source": [
    "# Remap 'SuccessfulCompletionState'\n",
    "This is important in case we want to use Success later for analysis etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "173a4db0dc3906e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:21:58.378490Z",
     "start_time": "2025-09-02T16:21:58.352616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid\n",
      "0037b2329de444c18d751b4e79901b39    {'all_nan': False, 'all_one': False, 'all_zero...\n",
      "0117810eb9634c4f98f842021ee6a595    {'all_nan': False, 'all_one': False, 'all_zero...\n",
      "0121f5b2f59d434f8beb17bf3e2a80b9    {'all_nan': False, 'all_one': False, 'all_zero...\n",
      "0956f0cca5f546d79a0cf4fbae23d496    {'all_nan': False, 'all_one': False, 'all_zero...\n",
      "09a23914cf354ea39444511406d16722    {'all_nan': False, 'all_one': False, 'all_zero...\n",
      "                                                          ...                        \n",
      "f9c6ff61370141c89ea9bbc536d796e1    {'all_nan': False, 'all_one': False, 'all_zero...\n",
      "fa2e2604ec6a4820851f032e80f09ba1    {'all_nan': False, 'all_one': False, 'all_zero...\n",
      "fa4ca90c5b80445b9af0b7ec4fbcc124    {'all_nan': False, 'all_one': False, 'all_zero...\n",
      "fd19a21a4a8846ca82bd127e4e1933f5    {'all_nan': False, 'all_one': False, 'all_zero...\n",
      "ff4288f304e74bbf93aa6508c7df8145    {'all_nan': False, 'all_one': False, 'all_zero...\n",
      "Name: SuccessfulCompletionState, Length: 160, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# To see if all values are NaN, only 1, only 0, or mixed\n",
    "result = df_single_event.groupby('uid')['SuccessfulCompletionState'].agg(\n",
    "    lambda x: {\n",
    "        'all_nan': x.isna().all(),\n",
    "        'all_one': (x == 1).all() if not x.isna().all() else False,\n",
    "        'all_zero': (x == 0).all() if not x.isna().all() else False,\n",
    "        'mixed': not (x.isna().all() or (x == 1).all() or (x == 0).all())\n",
    "    }\n",
    ")\n",
    "# To see the actual status info for each uid:\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad5c777b152c3691",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:22:05.707647Z",
     "start_time": "2025-09-02T16:22:05.689006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "        ... \n",
       "80155    1.0\n",
       "80156    1.0\n",
       "80157    1.0\n",
       "80158    1.0\n",
       "80159    1.0\n",
       "Name: SuccessfulCompletionState, Length: 80160, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Get the known value (0 or 1) for each uid\n",
    "# First, identify the non-NaN value per 'uid' (assumes consistent non-NaN)\n",
    "uid_value = df_single_event.groupby('uid')['SuccessfulCompletionState'].transform(lambda x: x.dropna().unique()[0])\n",
    "\n",
    "# 2. Fill NaNs with the corresponding uid's value\n",
    "# For entries where 'SuccessfulCompletionState' is NaN, replace with uid_value\n",
    "df_single_event['SuccessfulCompletionState'] = df_single_event['SuccessfulCompletionState'].fillna(uid_value)\n",
    "# To see if all values are NaN, only 1, only 0, or mixed\n",
    "df_single_event['SuccessfulCompletionState']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f69e589355f288",
   "metadata": {},
   "source": [
    "# 1. Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed2123f4f89bca90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:11:50.087396Z",
     "start_time": "2025-09-02T16:11:49.171127Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import scipy as sp\n",
    "from scipy.stats import chi2\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2\n",
    "from sklearn.covariance import MinCovDet\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    minmax_scale,\n",
    ")\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da18bb",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aac8a6882070d4f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:19:46.919046Z",
     "start_time": "2025-09-02T16:19:46.916060Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple mahalanobis distance\n",
    "def mahalanobis(df, no_dask=True, md_name='',outlier_name=''):\n",
    "    # Calculate mean\n",
    "    x_minus_mu = df - np.mean(df)\n",
    "    # Calculate covariance\n",
    "    cov = np.cov(df.values.T)\n",
    "    # Calculate inverse covariance\n",
    "    if df.shape[1] < 2:\n",
    "        # Reshape when only one column in used (e.g., 'SteeringInput')\n",
    "        inv_covmat = sp.linalg.inv(cov.reshape((1,1)))\n",
    "    else:\n",
    "        inv_covmat = sp.linalg.inv(cov)\n",
    "    if no_dask:\n",
    "        # No dask work only for individual UID\n",
    "        left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "        mahal = np.dot(left_term, x_minus_mu.T)\n",
    "        # Calculate md\n",
    "        md = np.sqrt(mahal.diagonal())\n",
    "    else:\n",
    "        # Dask needs to be used when calculating md across conditions\n",
    "        x_minus_mu_dask = da.from_array(x_minus_mu.to_numpy(), chunks=(min(x_minus_mu.shape[0], 10000), df.shape[1]))\n",
    "        mahal = np.sqrt(np.diagonal(np.dot(np.dot(x_minus_mu_dask,inv_covmat), x_minus_mu_dask.T)))\n",
    "        # Calculate md\n",
    "        md = mahal.compute()\n",
    "    # Save md values to df column\n",
    "    md_df = pd.DataFrame({md_name:md})\n",
    "    # Set a cut-off threshold based on the Chi2 and the degrees of freedom\n",
    "    # Here we choose to be conservative and only get very extreme outliers\n",
    "    threshold = chi2.ppf((1-0.001), df=df.shape[1])  # degrees of freedom = number of variables\n",
    "    # Assuming 'mahalanobis_distances' contains your computed distances\n",
    "    # mean_md = np.mean(md)\n",
    "    # std_md = np.std(md)\n",
    "    # threshold_5std = mean_md + 3 * std_md\n",
    "    # Set as outliers md > threshold\n",
    "    md_df[outlier_name] = md_df[md_name] > threshold\n",
    "    # if md_name == 'md_car':\n",
    "    #     print(C)\n",
    "    return md_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5524d9cfc809470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:19:47.961502Z",
     "start_time": "2025-09-02T16:19:47.957906Z"
    }
   },
   "outputs": [],
   "source": [
    "### Robust Mahalonibis Distance\n",
    "def robust_mahalanobis_method_dask(df, md_name='',outlier_name='', p_md_name='', cut=0.001):\n",
    "    #Minimum covariance determinant\n",
    "    rng = np.random.RandomState(0)\n",
    "    real_cov = np.cov(df.values.T)\n",
    "\n",
    "    # Calculate inverse covariance\n",
    "    if df.shape[1] < 2:\n",
    "        # Reshape when only one column in used (e.g., 'SteeringInput')\n",
    "        # X = rng.multivariate_normal(mean=np.mean(df, axis=0), cov=real_cov.reshape((1,1)), size= round(len(df) * 0.5))\n",
    "        cov = MinCovDet(random_state=0).fit(df) #calculate covariance\n",
    "        mcd = cov.covariance_ #robust covariance metric\n",
    "        robust_mean = cov.location_  #robust mean\n",
    "        inv_covmat = sp.linalg.inv(mcd) #inverse of covariance matrix\n",
    "    else:\n",
    "        X = rng.multivariate_normal(mean=np.mean(df, axis=0), cov=real_cov, size= round(len(df) * 0.5))\n",
    "        cov = MinCovDet(random_state=0).fit(X) #calculate covariance\n",
    "        mcd = cov.covariance_ #robust covariance metric\n",
    "        robust_mean = cov.location_  #robust mean\n",
    "        inv_covmat = sp.linalg.inv(mcd) #inverse of covariance matrix\n",
    "\n",
    "    # Robust M-Distance\n",
    "    x_minus_mu = df - robust_mean\n",
    "    print(x_minus_mu)\n",
    "    # Transform data into dask arrays\n",
    "    x_minus_mu_dask = da.from_array(x_minus_mu.to_numpy(), chunks=(min(x_minus_mu.shape[0], 10000), df.shape[1]))\n",
    "    mahal = da.sqrt(da.diagonal(da.dot(da.dot(x_minus_mu_dask,inv_covmat), x_minus_mu_dask.T)))\n",
    "    # Calculate md\n",
    "    md = mahal.compute()\n",
    "    # Compute the chi-squared cumulative probability distribution to transfer the md2 into probabilities\n",
    "    probability_md = 1 - chi2.cdf(md, df=df.shape[1])\n",
    "\n",
    "    # Save md values and probabilities to df column\n",
    "    md_df = pd.DataFrame({md_name:md,p_md_name:probability_md})\n",
    "    # Set a Chi2 cut-off point using probability of 0.01 (99.5% Chi2 quantile)\n",
    "    # Degrees of freedom (df) = number of variables\n",
    "    threshold = chi2.ppf((1-cut), df=df.shape[1]-1)\n",
    "    print(threshold)\n",
    "    # STD threshold, assuming 'md' contains your computed distances\n",
    "    # mean_md = np.mean(md)\n",
    "    # std_md = np.std(md)\n",
    "    # threshold_3std = mean_md + 4 * std_md\n",
    "    # Flag outliers as md > threshold\n",
    "    md_df[outlier_name] = md_df[md_name] > threshold\n",
    "    return md_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9c641a61ecc4de4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:19:51.677514Z",
     "start_time": "2025-09-02T16:19:51.675033Z"
    }
   },
   "outputs": [],
   "source": [
    "def mahalanobis_per_condition(df, cut=0.001):\n",
    "    # Select unique uids\n",
    "    conditions = df['ExperimentalCondition'].unique()\n",
    "\n",
    "    # Empty df to save all md and outliers\n",
    "    md_df = pd.DataFrame()\n",
    "\n",
    "    for i, condition in enumerate(conditions):\n",
    "        # Select the uid data subset\n",
    "        df_uid = df[df['ExperimentalCondition'].isin([condition])].reset_index(drop=True)\n",
    "        print(df_uid)\n",
    "        # ---- EYE columns ----\n",
    "        # mahalanobis distances for each data subset\n",
    "        mds_eye= robust_mahalanobis_method_dask(df=df[['EyeDirWorldCombined.x','EyeDirWorldCombined.y','EyeDirWorldCombined.z']], md_name='md_eye',outlier_name='eye_outlier',p_md_name='p_md_head', cut=cut).reset_index(drop=True)\n",
    "\n",
    "        ## ---- HEAD columns ----\n",
    "        mds_head= robust_mahalanobis_method_dask(df=df_uid[['NoseVector.x','NoseVector.y', 'NoseVector.z']], md_name='md_head_condition',outlier_name='head_outlier_condition', p_md_name='p_md_head_condition', cut=0.001).reset_index(drop=True)\n",
    "        #\n",
    "        # ## ---- CAR columns ----\n",
    "        # mds_car= robust_mahalanobis_method_dask(df=df_uid[['CarYaw_degrees', 'CarPitch_degrees', 'CarRoll_degrees']], md_name='md_car_condition',outlier_name='car_outlier_condition', p_md_name='p_md_car_condition', cut=0.001).reset_index(drop=True)\n",
    "        #\n",
    "        # ## ---- Steering ----\n",
    "        # mds_steer= robust_mahalanobis_method_dask(df=df_uid[['streeringDegree']], md_name='md_steering_condition',outlier_name='steering_outlier_condition', p_md_name='p_md_steer_condition', cut=0.001).reset_index(drop=True)\n",
    "        #\n",
    "        # ## ---- Pupil dilation ----\n",
    "        # mds_pupil= robust_mahalanobis_method_dask(df=df_uid[['Pupil_Dilation_mean']], md_name='md_pupil_dilation_condition',outlier_name='pupil_dilation_outlier_condition', p_md_name='p_md_pupil_condition', cut=0.001).reset_index(drop=True)\n",
    "\n",
    "        ## save md and outlier data\n",
    "        # md_df = pd.concat([md_df, pd.concat([mds_eye,mds_head,mds_car,mds_steer,mds_pupil], axis=1)], ignore_index=True)\n",
    "        md_df = pd.concat([md_df, pd.concat([mds_eye,mds_head], axis=1)], ignore_index=True)\n",
    "    ## Concatenate original df with calculated mahalanobis distances and outliers\n",
    "    # df = pd.concat([df.reset_index(drop=True), mds_eye.reset_index(drop=True)], axis=1)\n",
    "    df = pd.concat([df.reset_index(drop=True), md_df.reset_index(drop=True)], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54421bf3cc21fa99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:19:59.883991Z",
     "start_time": "2025-09-02T16:19:59.878857Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import scipy.linalg as sp_linalg\n",
    "from sklearn.covariance import MinCovDet\n",
    "from scipy.stats import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "\n",
    "def robust_mahalanobis_method_dask2(df, md_name='md', outlier_name='outlier', p_md_name='p_md', cut=0.001):\n",
    "    # Minimum covariance determinant\n",
    "    rng = np.random.RandomState(0)\n",
    "    real_cov = np.cov(df.values.T)\n",
    "\n",
    "    # Calculate inverse covariance robustly\n",
    "    # if df.shape[1] < 2:\n",
    "    #     cov = MinCovDet(random_state=0).fit(df.values.reshape(-1,1))\n",
    "    #     mcd = cov.covariance_\n",
    "    #     robust_mean = cov.location_\n",
    "    #     inv_covmat = sp_linalg.inv(mcd)\n",
    "    # else:\n",
    "    # X = rng.multivariate_normal(mean=np.mean(df, axis=0), cov=real_cov, size= round(len(df) * 0.5))\n",
    "    cov = MinCovDet(random_state=0).fit(df.values)\n",
    "    mcd = cov.covariance_\n",
    "    robust_mean = cov.location_\n",
    "    inv_covmat = sp_linalg.inv(mcd)\n",
    "\n",
    "    # Calculate robust Mahalanobis distances\n",
    "    x_minus_mu = df.values - robust_mean\n",
    "    x_minus_mu_dask = da.from_array(x_minus_mu, chunks=(min(x_minus_mu.shape[0], 10000), df.shape[1]))\n",
    "    mahal_sq = da.diagonal(x_minus_mu_dask @ inv_covmat @ x_minus_mu_dask.T)\n",
    "    md = da.sqrt(mahal_sq).compute()\n",
    "\n",
    "    # Calculate probability using Chi2 distribution\n",
    "    p_md = 1 - chi2.cdf(mahal_sq.compute(), df=df.shape[1])\n",
    "\n",
    "    # Threshold\n",
    "    threshold = chi2.ppf(1 - cut, df=df.shape[1])\n",
    "\n",
    "    # Compile result DataFrame\n",
    "    md_df = pd.DataFrame({\n",
    "        md_name: md,\n",
    "        p_md_name: p_md\n",
    "    })\n",
    "    md_df[outlier_name] = mahal_sq.compute() > threshold\n",
    "\n",
    "    return md_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5804e64435bec195",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f143895d97635d84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:20:44.122002Z",
     "start_time": "2025-09-02T16:20:44.119662Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mapping of old labels to new labels\n",
    "def map_labels(labels, label_mapping):\n",
    "    \"\"\"\n",
    "    Maps each label in the given list using the provided mapping dictionary.\n",
    "\n",
    "    Args:\n",
    "        labels (list): List of original labels.\n",
    "        label_mapping (dict): Mapping dictionary for label replacement.\n",
    "\n",
    "    Returns:\n",
    "        list: New list with mapped labels.\n",
    "    \"\"\"\n",
    "    return [label_mapping.get(label, label) for label in labels]\n",
    "\n",
    "# Define your label mapping (you can modify this as needed)\n",
    "custom_mapping = {\n",
    "    'EyeDirWorldCombined.x': 'Eye Horizontal',\n",
    "    'EyeDirWorldCombined.y': 'Eye Vertical',\n",
    "    'NoseVector.x': 'Head Horizontal',\n",
    "    'NoseVector.y': 'Head Vertical',\n",
    "    'SteeringInput': 'Steering',\n",
    "    'ExperimentalCondition ': 'Condition',\n",
    "    'SuccessfulCompletionState': 'SuccessfulCompletion'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22e9a777f5b8dc02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:20:45.649925Z",
     "start_time": "2025-09-02T16:20:45.640960Z"
    }
   },
   "outputs": [],
   "source": [
    "def biplot(score, coef, eigenvalues,\n",
    "           labels=None,\n",
    "           color_map=None,\n",
    "           colors=None,\n",
    "           explained_variance=None,\n",
    "           vector_colors=None,\n",
    "           scaled=None,\n",
    "           vector_linewidth=None,\n",
    "           legend_by='condition', # new parameter with default 'condition'\n",
    "           **kwargs,):\n",
    "    # plt.rcParams.update({'font.size': 10})\n",
    "    # Apply 90-degree rotation matrix to `score` and `coef`\n",
    "    rotation_matrix = np.array([[0, -1], [1, 0]])\n",
    "    # Rotate the first two components of the scores\n",
    "    # score = np.dot(score[:, :2], rotation_matrix)\n",
    "    # Rotate the first two components of the loadings\n",
    "    # coef = np.dot(coef[:, :2], rotation_matrix)\n",
    "    xs = score[:, 0]\n",
    "    ys = score[:, 1]\n",
    "    n = coef.shape[0]\n",
    "    scalex = 1.0 / (xs.max() - xs.min())\n",
    "    scaley = 1.0 / (ys.max() - ys.min())\n",
    "\n",
    "    padding= 1.2 # 20% padding for axis and vector scaling\n",
    "    padding_text = 1 # 10 % padding for text\n",
    "    xlims = padding * np.max(np.abs(xs))\n",
    "    ylims = padding * np.max(np.abs(ys))\n",
    "    if colors is None:  # If no color information, plot all points orange\n",
    "        plt.scatter(xs, ys, s=80, color='gray', alpha=0.5,edgecolor='gray')\n",
    "        # plt.scatter(xs, ys, s=80, color='orange')\n",
    "    else:  # If color information is given, plot points with corresponding colors\n",
    "        # plt.scatter(xs * scalex, ys * scaley, s=80, color=colors, alpha=0.5)\n",
    "        plt.scatter(xs, ys, s=80, color=colors, alpha=0.5,edgecolor=colors)\n",
    "    plt.xlim(-xlims * padding,xlims * padding)\n",
    "    plt.ylim(-ylims* padding,ylims* padding)\n",
    "\n",
    "    # Adjust the number of labels to match the number of coefficients\n",
    "    original_labels = labels[:n]\n",
    "\n",
    "    # Call the function to get the new mapped labels\n",
    "    labels = map_labels(original_labels, custom_mapping)\n",
    "\n",
    "    # Draw principal component vectors as arrows\n",
    "    for i in range(n):\n",
    "        # Calculate arrow end point (arrow head)\n",
    "        arrow_end_x = xlims * coef[i, 0]\n",
    "        arrow_end_y = ylims * coef[i, 1]\n",
    "        if scaled:\n",
    "            # Draw the arrow starting from the origin (0, 0) to the arrow head\n",
    "            plt.arrow(0, 0, arrow_end_x, arrow_end_y, color=vector_colors[i], alpha=1, linewidth=vector_linewidth,length_includes_head=True,head_length=0.1,head_width=0.1,overhang=0)\n",
    "        else:\n",
    "            plt.arrow(0, 0, arrow_end_x, arrow_end_y, color=vector_colors[i], alpha=1, linewidth=vector_linewidth,length_includes_head=True,head_length=0.7,head_width=0.7,overhang=0)\n",
    "    # Position the label above the arrow head\n",
    "        if labels is not None:\n",
    "            if arrow_end_y > 0:\n",
    "                va='bottom'\n",
    "            else:\n",
    "                va='top'\n",
    "            # Annotate the point with text (you can customize the text as needed)\n",
    "            plt.text(arrow_end_x * padding_text, arrow_end_y * padding_text,\n",
    "                     f\"{labels[i]}\\n({coef[i, 0]:.2f}, {coef[i, 1]:.2f})\",color='black',ha='center', va=va,fontdict=dict(fontsize=14), bbox=dict(facecolor='white', alpha=0.0001, edgecolor='white'))\n",
    "    # Write first two eigenvalues\n",
    "    # plt.text(-ylims + padding_text, -xlims * padding_text, \"Eigenvalues:\\n {:.2f}, {:.2f}\".format(*eigenvalues),ha='left', va='bottom', fontsize=10)\n",
    "    # Write all eigenvalues\n",
    "    # plt.text(-ylims + 0.5, -xlims + 0.5, \"Eigenvalues:\\n \" + \", \".join(f\"{val:.2f}\" for val in eigenvalues), ha='left', va='bottom', fontsize=10)\n",
    "\n",
    "    # Set fixed axis limits\n",
    "    ax = plt.gca()\n",
    "    fixed_x_min, fixed_x_max = -xlims, xlims  # Your specified x limits\n",
    "    fixed_y_min, fixed_y_max = -ylims, ylims  # Your specified y limits\n",
    "\n",
    "    # Set the fixed limits\n",
    "    plt.xlim(fixed_x_min, fixed_x_max)\n",
    "    plt.ylim(fixed_y_min, fixed_y_max)\n",
    "\n",
    "    # Hide all spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "    # Parameters for arrows\n",
    "    arrow_length = 0.15  # Size of the arrowhead\n",
    "    arrow_linewidth = 2.5\n",
    "\n",
    "    # Create y-axis arrow (vertical) ON THE LEFT SPINE\n",
    "    #  Start at bottom left corner\n",
    "    ax.arrow(fixed_x_min +arrow_length, fixed_y_min+arrow_length,\n",
    "             0, fixed_y_max - (fixed_y_max/2.5),  # Go up to max - arrowhead space\n",
    "             head_width=arrow_length, head_length=arrow_length,\n",
    "             fc='black', ec='black', linewidth=arrow_linewidth,\n",
    "             length_includes_head=True)\n",
    "\n",
    "    # Create x-axis arrow (horizontal) ON THE BOTTOM SPINE\n",
    "    # Start at bottom left corner\n",
    "    ax.arrow(fixed_x_min +arrow_length, fixed_y_min+arrow_length,\n",
    "             fixed_x_max - (fixed_x_max/2.5), 0,  # Go right to max - arrowhead space\n",
    "             head_width=arrow_length, head_length=arrow_length,\n",
    "             fc='black', ec='black', linewidth=arrow_linewidth,\n",
    "             length_includes_head=True)\n",
    "    # Hide tick parameters\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Remove standard labels\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "    # Add custom positioned labels\n",
    "    # X-axis label - at the start of the x-axis arrow\n",
    "    ax.text(fixed_x_min + 1.5, fixed_y_min - 0.1,  # Position just below the arrow end\n",
    "            \"PC{} ({:.1f}%)\".format(1, explained_variance[0] * 100),\n",
    "            fontsize=10, ha='right', va='top')\n",
    "\n",
    "    # Y-axis label - at the start of the y-axis arrow\n",
    "    ax.text(fixed_x_min + 0.02, fixed_y_min + 0.6,  # Position to the left of arrow end\n",
    "            \"PC{} ({:.1f}%)\".format(2, explained_variance[1] * 100),\n",
    "            fontsize=10, ha='right', va='bottom', rotation=90)\n",
    "\n",
    "    # Add a legend for the colors if provided\n",
    "    # Create a mapping from condition to color\n",
    "    # Add a legend for the condition labels based on their colors\n",
    "    # Automatic legend based on color_map\n",
    "    # Generate legend based on specified criteria\n",
    "    if colors is not None:\n",
    "        import collections\n",
    "        handles = []\n",
    "\n",
    "        if legend_by == 'condition' and color_map is not None:\n",
    "            # Build reverse map from color to condition\n",
    "            color_to_condition = {v: k for k, v in color_map.items()}\n",
    "            unique_colors = list({c for c in colors})\n",
    "            for col in unique_colors:\n",
    "                label = color_to_condition.get(col, col)\n",
    "                handle = Line2D([0], [0], marker='o', color='w',\n",
    "                                markerfacecolor=col, markersize=10,\n",
    "                                label=label)\n",
    "                handles.append(handle)\n",
    "            plt.legend(handles=handles, title = legend_by.capitalize(), fontsize=10, title_fontsize= 12, loc='upper right')\n",
    "\n",
    "\n",
    "        elif legend_by == 'completion':\n",
    "            # Based on 'SuccessfulCompletionState' column\n",
    "            # Create a mapping for completion status if not provided\n",
    "            # Assign green for success (==1), red otherwise\n",
    "            for cond_value in set(kwargs.get('completion_states', [])):\n",
    "                if cond_value == 0:\n",
    "                    col = 'red'\n",
    "                    label = 'Failure'\n",
    "                elif cond_value == 1:\n",
    "                    col = 'green'\n",
    "                    label = 'Success'\n",
    "                handle = Line2D([0], [0], marker='o', color='w',\n",
    "                                markerfacecolor=col, markersize=10,\n",
    "                                label=label)\n",
    "                handles.append(handle)\n",
    "            plt.legend(handles=handles,title = legend_by.capitalize(), fontsize=10, title_fontsize= 12, loc='upper right')\n",
    "            # sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1), fontsize=10)\n",
    "    # plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2dbdd0219f32749d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:20:46.562384Z",
     "start_time": "2025-09-02T16:20:46.559702Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pca_var(pca, subset_scaled, feature_names):\n",
    "    # Validate input types\n",
    "    if not isinstance(pca, PCA):\n",
    "        raise TypeError(\"Expected a PCA object from sklearn.decomposition.PCA\")\n",
    "\n",
    "    # Compute coordinates\n",
    "    coords = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "    # Compute correlations\n",
    "    # Since we're using standardized data:\n",
    "    cor = coords / np.std(subset_scaled, axis=0)\n",
    "\n",
    "    # Compute cos2 (squared loadings or cosine similarity)\n",
    "    cos2 = np.square(cor)\n",
    "\n",
    "    # Compute contributions\n",
    "    total_variance = np.sum(pca.explained_variance_)\n",
    "    contrib = (cos2 * 100 * pca.explained_variance_) / total_variance\n",
    "\n",
    "    # Create dataframes with feature names as the index\n",
    "    coord_df = pd.DataFrame(coords, index=feature_names, columns=[f'PC{i+1}' for i in range(pca.n_components_)])\n",
    "    cor_df = pd.DataFrame(cor, index=feature_names, columns=[f'PC{i+1}' for i in range(pca.n_components_)])\n",
    "    cos2_df = pd.DataFrame(cos2, index=feature_names, columns=[f'PC{i+1}' for i in range(pca.n_components_)])\n",
    "    contrib_df = pd.DataFrame(contrib, index=feature_names, columns=[f'PC{i+1}' for i in range(pca.n_components_)])\n",
    "\n",
    "    return {\n",
    "        'coord': coord_df,\n",
    "        'cor': cor_df,\n",
    "        'cos2': cos2_df,\n",
    "        'contrib': contrib_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b02ad314181c3d60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:20:47.415116Z",
     "start_time": "2025-09-02T16:20:47.410321Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle\n",
    "from matplotlib import colors as mcolors\n",
    "\n",
    "def plot_cosine_similarity(cos2_df, title='', time_point='', save=False):\n",
    "    # Extract feature names directly from the DataFrame index\n",
    "    feature_names = cos2_df.index\n",
    "\n",
    "    # max_cos2_value = 1\n",
    "    # print(f'Plotting cosine similarity...')\n",
    "    max_cos2_value = cos2_df.values.max() # color bar maximum limit\n",
    "    # Create a custom colormap from white to #A2530E\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list(\"\", [\"white\", \"#A2530E\"])\n",
    "\n",
    "    features, components = cos2_df.shape\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    if len(feature_names) > 10:\n",
    "        num_fontsize = 10\n",
    "    num_fontsize = 12\n",
    "    ax.set_title(title, fontsize=18)\n",
    "\n",
    "    # Plot each circle and add annotations\n",
    "    for i in range(features):\n",
    "        for j in range(components):\n",
    "            value = cos2_df.iloc[i, j]\n",
    "            color = cmap(value / max_cos2_value)  # Normalize the color\n",
    "            circle = Circle((j, i), radius=np.sqrt(value) * 0.5, color=color, fill=True)\n",
    "            ax.add_artist(circle)\n",
    "\n",
    "            # Annotate the cos2 value at the center of the circle\n",
    "            ax.text(\n",
    "                j, i, f'{value:.2f}',\n",
    "                color='lightgray',\n",
    "                ha='center', va='center',\n",
    "                fontsize=num_fontsize,  # Adjust font size for readability\n",
    "            )\n",
    "\n",
    "    # Setup axis limits and labels\n",
    "    ax.set_xlim(-0.5, components - 0.5)\n",
    "    ax.set_ylim(-0.5, features - 0.5)\n",
    "    ax.set_xticks(np.arange(components))\n",
    "    ax.set_yticks(np.arange(features))\n",
    "    ax.set_xticklabels([f'PC{i+1}' for i in range(components)], rotation=0)\n",
    "    ax.set_yticklabels(feature_names)\n",
    "    ax.set_aspect('equal', 'box')\n",
    "\n",
    "    # Add grid lines for clarity\n",
    "    ax.hlines(np.arange(-0.5, features), xmin=-0.5, xmax=components - 0.5, color='grey', lw=0.5)\n",
    "    ax.vlines(np.arange(-0.5, components), ymin=-0.5, ymax=features - 0.5, color='grey', lw=0.5)\n",
    "\n",
    "    # Set outer border to gray by modifying the spines\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('grey')\n",
    "        spine.set_linewidth(0.5)\n",
    "\n",
    "    # Add a color bar on the right\n",
    "    norm = mcolors.Normalize(vmin=0, vmax=max_cos2_value)\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, orientation=\"vertical\", fraction=0.046, pad=0.04, label='$Cos^2$')\n",
    "\n",
    "    # Increase the number of ticks on the color bar for better granularity\n",
    "    num_ticks = 6\n",
    "    cbar.set_ticks(np.linspace(0, max_cos2_value, num_ticks))\n",
    "    cbar.set_ticklabels([round(val, 2) for val in np.linspace(0, max_cos2_value, num_ticks)])\n",
    "    cbar.outline.set_visible(True)\n",
    "    plt.tight_layout()\n",
    "    # if save:\n",
    "    #     plt.savefig(f\"plots/cosine/pca_cos2_{event}_{time_point}_{''.join(map(str, feature_names))}.pdf\", dpi=1200)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b800ac612bb6557b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:20:49.027904Z",
     "start_time": "2025-09-02T16:20:49.016224Z"
    }
   },
   "outputs": [],
   "source": [
    "# Directories to save results\n",
    "# Ensure os is imported to work with the file system\n",
    "import os\n",
    "from tqdm import tqdm  # Import the tqdm library for progress bar\n",
    "import time  # Import time for simulating or handling elapsed time calculation\n",
    "\n",
    "# Define the base results path\n",
    "results_path = \"data/pca_results\"\n",
    "\n",
    "# Create base results directory\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "# Create subdirectories under the base results path\n",
    "directories = {\n",
    "    \"cleaned_new\": os.path.join(results_path, \"cleaned_new\"),\n",
    "    \"variances\": os.path.join(results_path, \"variances\"),\n",
    "    \"components\": os.path.join(results_path, \"components\"),\n",
    "    \"eigenvalues\": os.path.join(results_path, \"eigenvalues\"),\n",
    "}\n",
    "\n",
    "# Create each subdirectory\n",
    "for subdir in directories.values():\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "def visualize_event_pca(df, event='', time_point='', d_ts = 0, features='', scaled=True, vis_biplot=True,output_mode = 'show', legend_mode='condition',save_biplot=False, save_cos2_plot=False, save_cleaned=False, direc_path=''):\n",
    "    if not event == '':\n",
    "        # Find the actual event name in the dataframe that contains the provided event name\n",
    "        available_events = df['EventName'].unique()\n",
    "        matching_events = [e for e in available_events if event in e]\n",
    "        \n",
    "        if not matching_events:\n",
    "            raise ValueError(f\"No event found containing '{event}'. Available events: {available_events}\")\n",
    "        elif len(matching_events) > 1:\n",
    "            print(f\"Warning: Multiple events found containing '{event}': {matching_events}. Using the first one: {matching_events[0]}\")\n",
    "        \n",
    "        actual_event_name = matching_events[0]\n",
    "        print(f\"Using event: {actual_event_name}\")\n",
    "        \n",
    "        # Filter data for the actual event\n",
    "        event_df = df[df['EventName'] == actual_event_name]\n",
    "        \n",
    "        if not time_point == '':\n",
    "            if time_point == 'start':\n",
    "                # event start time\n",
    "                ts = event_df['time_from_event'].min()\n",
    "            elif time_point == 'end':\n",
    "                # event end time\n",
    "                ts = event_df['time_from_event'].max()\n",
    "            elif time_point == 'onset':\n",
    "                # event end time\n",
    "                ts = 0\n",
    "            elif time_point == 'dynamic':\n",
    "                ts = d_ts\n",
    "            # timestamps = event_df['time'].unique()\n",
    "            timestamps = event_df[np.isclose(event_df['time_from_event'], ts, atol=0.00001)]['time_from_event'].unique()\n",
    "        else:\n",
    "            # timestamp subset of the entire event\n",
    "            timestamps = event_df['time_from_event'].unique()\n",
    "    else:\n",
    "        event = 'all_events'\n",
    "        # timestamp subset of the entire df\n",
    "        timestamps = df['time_from_event'].unique()\n",
    "\n",
    "    # DataFrames to save data\n",
    "    df_cleaned_ts = pd.DataFrame()\n",
    "    df_event_components = pd.DataFrame()\n",
    "    df_event_variance = pd.DataFrame()\n",
    "    df_pca_results = pd.DataFrame()\n",
    "    df_sorted_eigenvalues = pd.DataFrame()  # To store sorted eigenvalues\n",
    "\n",
    "    with (tqdm(total=len(timestamps), desc=\"Processing times\", unit=\"\", colour=\"green\") as pbar):\n",
    "        start_time = time.time()  # Store the starting time\n",
    "        for i, timestamp in enumerate(timestamps):\n",
    "            # Clear the previous plot before drawing the new one\n",
    "            # ax.clear()\n",
    "            # 0. Extract rows for this timestamp\n",
    "            subset = df[df['time_from_event'] == timestamp]\n",
    "            event_name = subset['EventName'].unique()[0]\n",
    "\n",
    "            # 1. Update progress bar for the current timestamp\n",
    "            pbar.set_postfix_str(f\"Current Timestamp: {timestamp}\")\n",
    "\n",
    "            # 2. Clean outliers in subset\n",
    "            subset_cleaned = subset#.pipe(clean_subset)\n",
    "\n",
    "            # 3. Features\n",
    "            subset_features1 = subset_cleaned[subset_cleaned.columns.intersection(features)]\n",
    "            subset_features_renamed = subset_features1.rename(columns=custom_mapping)\n",
    "            subset_features = subset_features_renamed.drop(columns=['ExperimentalCondition','SuccessfulCompletion'])\n",
    "            original_labels = list(subset_features.columns)\n",
    "\n",
    "            completion_states = list(subset['SuccessfulCompletionState'].unique())\n",
    "\n",
    "            # 3.1 Colors\n",
    "            if legend_mode == 'completion':\n",
    "                color_map = {\n",
    "                    1.0 : 'green',\n",
    "                    0.0 : 'red',\n",
    "                }\n",
    "                colors = [color_map.get(condition, 'black') for condition in subset_features_renamed['SuccessfulCompletion']]\n",
    "            else:\n",
    "                color_map = {\n",
    "                        'BaseCondition': 'gray',\n",
    "                        'FullLoopAR': '#cc2936',\n",
    "                        'HUDOnly': '#1f77b4',   # example blue\n",
    "                        'AudioOnly': '#ff7f0e'  # example orange\n",
    "                    }\n",
    "\n",
    "                colors = [color_map.get(condition, 'black') for condition in subset_features_renamed['ExperimentalCondition']]\n",
    "\n",
    "            # 4. Scaling\n",
    "            if scaled:\n",
    "                scaler = StandardScaler(with_std=True)\n",
    "                subset_scaled = scaler.fit_transform(subset_features)\n",
    "            else:\n",
    "                subset_scaled = subset_features\n",
    "\n",
    "            # 5. Perform PCA\n",
    "            pca = PCA(whiten=True)#, svd_solver='full')\n",
    "            pca_result = pca.fit_transform(subset_scaled)\n",
    "            explained_variance = pca.explained_variance_ratio_\n",
    "            eigenvalues = pca.explained_variance_\n",
    "\n",
    "            # 5.1 Sorted eigenvalues\n",
    "            sorted_eigenvalue_columns = [f'eigen_val{i+1}' for i in range(len(eigenvalues))]\n",
    "            eigenvalues_df = pd.DataFrame([eigenvalues], columns=sorted_eigenvalue_columns)\n",
    "            eigenvalues_df['Time'] = timestamp\n",
    "            eigenvalues_df['EventName'] = event_name\n",
    "            df_sorted_eigenvalues = pd.concat([df_sorted_eigenvalues, eigenvalues_df], ignore_index=True)\n",
    "\n",
    "            # Save PCA results\n",
    "            pca_result_df = pd.DataFrame(\n",
    "                pca_result,\n",
    "                columns=[f'PC{i+1}' for i in range(len(explained_variance))]\n",
    "            )\n",
    "            pca_result_df['Time'] = timestamp\n",
    "            pca_result_df['EventName'] = event_name\n",
    "            df_pca_results = pd.concat([df_pca_results, pca_result_df], ignore_index=True)\n",
    "\n",
    "            # Components\n",
    "            loadings = pd.DataFrame(\n",
    "                pca.components_.T,\n",
    "                columns=[f'PC{i+1}' for i, v in enumerate(explained_variance)],\n",
    "            )\n",
    "            loadings['Features'] = original_labels\n",
    "            loadings['Time'] = timestamp\n",
    "            loadings['EventName'] = event_name\n",
    "            df_event_components = pd.concat([df_event_components, loadings], ignore_index=True)\n",
    "\n",
    "            # Variance\n",
    "            df_variances = pd.DataFrame(\n",
    "                np.reshape(explained_variance, (1, len(explained_variance))),\n",
    "                columns=[f'PC{i+1}' for i, v in enumerate(explained_variance)]\n",
    "            )\n",
    "            df_variances['Time'] = timestamp\n",
    "            df_variances['EventName'] = event_name\n",
    "            df_event_variance = pd.concat([df_event_variance, df_variances], ignore_index=True)\n",
    "\n",
    "            # Visualization\n",
    "            if vis_biplot:\n",
    "                sns.set_theme(style=\"white\")\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.title('Second: ' + str(timestamp), fontsize=22)\n",
    "                biplot(\n",
    "                    pca_result,\n",
    "                    pca.components_.T,\n",
    "                    eigenvalues,\n",
    "                    original_labels,\n",
    "                    color_map,\n",
    "                    colors,\n",
    "                    explained_variance,\n",
    "                    vector_colors=['k'] * len(original_labels),\n",
    "                    scaled=scaled,\n",
    "                    vector_linewidth=1.1,\n",
    "                    legend_by=legend_mode,\n",
    "                    completion_states=completion_states\n",
    "                )\n",
    "                # --- Display or save biplot ---\n",
    "                if output_mode == 'save':\n",
    "                    if save_biplot:\n",
    "                        save_path = f\"plots/biplot/pca_{event}_{timestamp}_{''.join(map(str, original_labels))}.png\"\n",
    "                        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                        plt.savefig(save_path, dpi=1200)\n",
    "                        # print(f\"Biplot saved to {save_path}\")\n",
    "                    plt.close() # Always close in save mode\n",
    "                elif output_mode == 'show':\n",
    "                    plt.show() # Show the plot and wait for user to close it\n",
    "                else:\n",
    "                    plt.close() # Default action is to close to prevent memory leaks\n",
    "                    # print(\"Biplot created, saved, and closed.\")\n",
    "\n",
    "            # --- Block for Cosine Similarity Plot ---\n",
    "            # plt.figure(figsize=(8, 4)) # Creates Figure B, makes it active\n",
    "            # plt.close()\n",
    "            pca_cos2 = get_pca_var(pca, subset_scaled, original_labels)\n",
    "            cos2_matrix = pca_cos2['cos2']\n",
    "            plot_cosine_similarity(cos2_matrix, title='Cosine Similarity ($Cos^2$)') # Plots on Figure B\n",
    "\n",
    "            # --- Display or save cosine plot ---\n",
    "            if output_mode == 'save':\n",
    "                if save_cos2_plot:\n",
    "                    save_path = f\"plots/cosine/pca_cos2_{event}_{timestamp}_{''.join(map(str, original_labels))}.png\"\n",
    "                    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                    plt.savefig(save_path, dpi=1200)\n",
    "                    # print(f\"Cosine similarity plot saved to {save_path}\")\n",
    "                plt.close() # Always close in save mode\n",
    "            elif output_mode == 'show':\n",
    "                plt.show() # Show the plot\n",
    "            else:\n",
    "                plt.close() # Default action is to close\n",
    "                # print(\"Cosine plot created and closed.\")\n",
    "            plt.close()\n",
    "            # finally, capture this frame\n",
    "            # camera.snap()\n",
    "\n",
    "            # Cleaned subset\n",
    "            if save_cleaned:\n",
    "                subset_features_renamed['EventName'] = event_name\n",
    "                subset_features_renamed['Time'] = timestamp\n",
    "                df_cleaned_ts = pd.concat([df_cleaned_ts, subset_features_renamed], ignore_index=True)\n",
    "\n",
    "            # Update progress bar and elapsed time\n",
    "            elapsed_time = time.time() - start_time\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix_str(f\"Elapsed: {elapsed_time:.2f}s\")\n",
    "\n",
    "    # Save results\n",
    "    if save_cleaned:\n",
    "        # Determine the filename suffix based on whether we're processing a specific event or all events\n",
    "        if event != 'all_events':\n",
    "            # For specific events, use the provided event name and 'all' timestamps\n",
    "            timestamp_suffix = 'all'\n",
    "            event_suffix = event  # Use the simplified event name provided by user\n",
    "        else:\n",
    "            # For all events, use 'all_events' and 'all' timestamps\n",
    "            timestamp_suffix = 'all'\n",
    "            event_suffix = 'all_events'\n",
    "        \n",
    "        df_cleaned_ts.to_csv(f\"{direc_path['cleaned_new']}/cleaned_event_{event_suffix}_timestamp_{timestamp_suffix}_{len(original_labels)}_variables_.csv\", index=False)\n",
    "        df_event_variance.to_csv(f\"{direc_path['variances']}/variances_event_{event_suffix}_timestamp_{timestamp_suffix}_{len(original_labels)}_variables.csv\", index=False)\n",
    "        df_event_components.to_csv(f\"{direc_path['components']}/components_event_{event_suffix}_timestamp_{timestamp_suffix}_{len(original_labels)}_variables.csv\", index=False)\n",
    "        df_sorted_eigenvalues.to_csv(f\"{direc_path['eigenvalues']}/eigenval_event_{event_suffix}_timestamp_{timestamp_suffix}_{len(original_labels)}_variables.csv\", index=False)\n",
    "    # After the loop – create and display the animation\n",
    "    # animation = camera.animate(blit=False, interval=500)\n",
    "    # display(HTML(animation.to_html5_video()))\n",
    "    return df_event_components, df_event_variance, original_labels, df_cleaned_ts, df_pca_results, df_sorted_eigenvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adf261d839b8f4a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:20:51.483350Z",
     "start_time": "2025-09-02T16:20:51.481168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp_REF', 'uid', 'dataset', 'city_section',\n",
       "       'ExperimentalCondition', 'UnixTimeStamp', 'TobiiTimeStamp',\n",
       "       'RightEyeIsBlinkingWorld', 'RightEyeIsBlinkingLocal',\n",
       "       'LeftEyeIsBlinkingWorld', 'LeftEyeIsBlinkingLocal', 'HmdPosition.x',\n",
       "       'HmdPosition.y', 'HmdPosition.z', 'NoseVector.x', 'NoseVector.y',\n",
       "       'NoseVector.z', 'EyePosWorldCombined.x', 'EyePosWorldCombined.y',\n",
       "       'EyePosWorldCombined.z', 'EyeDirWorldCombined.x',\n",
       "       'EyeDirWorldCombined.y', 'EyeDirWorldCombined.z',\n",
       "       'EyePosLocalCombined.x', 'EyePosLocalCombined.y',\n",
       "       'EyePosLocalCombined.z', 'EyeDirLocalCombined.x',\n",
       "       'EyeDirLocalCombined.y', 'EyeDirLocalCombined.z', 'ObjectName_1',\n",
       "       'ObjectName_2', 'ObjectName_3', 'ObjectName_4', 'ObjectName_5',\n",
       "       'HitObjectPosition.x_1', 'HitObjectPosition.x_2',\n",
       "       'HitObjectPosition.x_3', 'HitObjectPosition.x_4',\n",
       "       'HitObjectPosition.x_5', 'HitObjectPosition.y_1',\n",
       "       'HitObjectPosition.y_2', 'HitObjectPosition.y_3',\n",
       "       'HitObjectPosition.y_4', 'HitObjectPosition.y_5',\n",
       "       'HitObjectPosition.z_1', 'HitObjectPosition.z_2',\n",
       "       'HitObjectPosition.z_3', 'HitObjectPosition.z_4',\n",
       "       'HitObjectPosition.z_5', 'HitPointOnObject.x_1', 'HitPointOnObject.x_2',\n",
       "       'HitPointOnObject.x_3', 'HitPointOnObject.x_4', 'HitPointOnObject.x_5',\n",
       "       'HitPointOnObject.y_1', 'HitPointOnObject.y_2', 'HitPointOnObject.y_3',\n",
       "       'HitPointOnObject.y_4', 'HitPointOnObject.y_5', 'HitPointOnObject.z_1',\n",
       "       'HitPointOnObject.z_2', 'HitPointOnObject.z_3', 'HitPointOnObject.z_4',\n",
       "       'HitPointOnObject.z_5', 'distanceToPlayer_1', 'distanceToPlayer_2',\n",
       "       'distanceToPlayer_3', 'distanceToPlayer_4', 'distanceToPlayer_5',\n",
       "       'EventName', 'StartofEventTimeStamp', 'EndOfEventTimeStamp',\n",
       "       'EventDuration', 'SuccessfulCompletionState', 'SteeringInput',\n",
       "       'AcellerationInput', 'BrakeInput', 'ReceivedInput', 'TimeStamp',\n",
       "       'TimeStamp_in_sec', 'TimeDiff_in_sec', 'Blink', 'BlinkDuration',\n",
       "       'BlinkDurationsLong', 'BlinkAfter', 'SamplingRate', 'time_from_event',\n",
       "       'outlier'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_single_event.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45629ad0585f69e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:20:53.838408Z",
     "start_time": "2025-09-02T16:20:53.836482Z"
    }
   },
   "outputs": [],
   "source": [
    "conditions = ['ExperimentalCondition','SuccessfulCompletionState']\n",
    "eye = ['EyeDirWorldCombined.x', 'EyeDirWorldCombined.y']#, 'EyeDirLocalCombined.z']\n",
    "head = ['NoseVector.x', 'NoseVector.y']#, 'NoseVector.z']\n",
    "steering = ['SteeringInput']\n",
    "variables_to_use = conditions + eye + head #+ steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9db85c78b5cfb5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:21:00.314957Z",
     "start_time": "2025-09-02T16:21:00.126379Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "import ipywidgets as widgets  # interactive display\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcb5adcfbe136c3",
   "metadata": {},
   "source": [
    "# Saving data\n",
    "1. You can work with 1 **event** by using the name parameter `event = 'StagEventNew'`, or all the entire data by `event = ''`\n",
    "\n",
    "2. In case you want to **save data**, then change parameter `output_mode = 'save'` in the function and\n",
    "--> `save_biplot=True`, `save_cos2_plot=True`, and `save_cleaned=True`, `vis_biplot=False`\n",
    "3. If you want to **visualize by* either successful completion state `legend_mode= 'completion'`, otherwise use `legend_mode= 'condition'`\n",
    "4. Look for the results data in the respective folders and load it for the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ded9b6949a85df31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:25:16.320154Z",
     "start_time": "2025-09-02T16:25:16.157033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving complete event data...\n",
      "Using event: StagEventNew\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing times: 100%|\u001b[32m██████████\u001b[0m| 501/501 [00:17<00:00, 29.18/s, Elapsed: 17.17s]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete event data saved successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf25c17eb881479da1ba1b42efdde3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.2, description='Time', layout=Layout(width='800px'), max=5.0, min=-5…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive_pca(d_ts)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets  # Add import if not already present\n",
    "\n",
    "event = 'StagEventNew'  # Process all data in the dataframe\n",
    "time_point = 'dynamic'\n",
    "scaled = True\n",
    "timestamps = sorted(df_single_event['time_from_event'].unique())\n",
    "\n",
    "# Create global variables to store outputs\n",
    "components_df = None\n",
    "variances_df = None\n",
    "original_labels = None\n",
    "df_cleaned_ts = None\n",
    "df_pca_results = None\n",
    "eigenvalues_df = None\n",
    "\n",
    "# First, save the complete event data (this happens once, independent of the widget)\n",
    "print(\"Saving complete event data...\")\n",
    "components_df, variances_df, original_labels, df_cleaned_ts, df_pca_results, eigenvalues_df = visualize_event_pca(\n",
    "    df_single_event,\n",
    "    event=event,\n",
    "    time_point='',  # Process all timestamps for saving\n",
    "    d_ts=0,  # Not used when time_point is empty\n",
    "    features=variables_to_use,\n",
    "    scaled=scaled,\n",
    "    vis_biplot=False,  # No visualization for saving\n",
    "    output_mode='save',\n",
    "    legend_mode='completion',\n",
    "    save_biplot=False,  # No individual biplots for saving\n",
    "    save_cos2_plot=False,  # No individual cos2 plots for saving\n",
    "    save_cleaned=True,  # Save the complete dataset\n",
    "    direc_path=directories\n",
    ")\n",
    "print(\"Complete event data saved successfully!\")\n",
    "\n",
    "# Now create the interactive widget for visualization only\n",
    "def interactive_pca(d_ts):\n",
    "    # This function only handles visualization, no saving\n",
    "    visualize_event_pca(\n",
    "        df_single_event,\n",
    "        event=event,\n",
    "        time_point=time_point,\n",
    "        d_ts=d_ts, # dynamic timestamp for slider\n",
    "        features=variables_to_use,\n",
    "        scaled=scaled,\n",
    "        vis_biplot=True,\n",
    "        output_mode='show',\n",
    "        legend_mode='completion',\n",
    "        save_biplot=False,  # No saving in widget\n",
    "        save_cos2_plot=False,  # No saving in widget\n",
    "        save_cleaned=False,  # No saving in widget\n",
    "        direc_path=directories\n",
    "    )\n",
    "\n",
    "widgets.interact(\n",
    "    interactive_pca,\n",
    "    d_ts=widgets.FloatSlider(\n",
    "        value=0.2, min=-5, max=5, step=0.02,\n",
    "        description='Time',\n",
    "        layout=widgets.Layout(width='800px')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d83b5caa8f40d26",
   "metadata": {},
   "source": [
    "# Visualize Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75036617",
   "metadata": {},
   "source": [
    "1. Currently, the variables `components_df,components_df, variances_df, original_labels, df_cleaned_ts, df_pca_results, eigenvalues_df` contain all the results information for the selected event. If empty `event = '' `, then for all data.\n",
    "\n",
    "2. Since these variables are being held in memory, you can use them to visualize the variances and effective dimensionality. To avoind having to run the whole script multiple times (when restarting the kernel, etc.) then, you can also load the data from the results folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49bf9fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>Features</th>\n",
       "      <th>Time</th>\n",
       "      <th>EventName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.697631</td>\n",
       "      <td>0.166558</td>\n",
       "      <td>-0.169519</td>\n",
       "      <td>-0.675894</td>\n",
       "      <td>Head Horizontal</td>\n",
       "      <td>-5.00</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.710770</td>\n",
       "      <td>0.040089</td>\n",
       "      <td>-0.189767</td>\n",
       "      <td>-0.676156</td>\n",
       "      <td>Head Vertical</td>\n",
       "      <td>-5.00</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.054723</td>\n",
       "      <td>0.711631</td>\n",
       "      <td>-0.642056</td>\n",
       "      <td>0.279913</td>\n",
       "      <td>Eye Horizontal</td>\n",
       "      <td>-5.00</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.071563</td>\n",
       "      <td>-0.681347</td>\n",
       "      <td>-0.723199</td>\n",
       "      <td>0.087346</td>\n",
       "      <td>Eye Vertical</td>\n",
       "      <td>-5.00</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.711701</td>\n",
       "      <td>0.098029</td>\n",
       "      <td>-0.161178</td>\n",
       "      <td>-0.676679</td>\n",
       "      <td>Head Horizontal</td>\n",
       "      <td>-4.98</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-0.471066</td>\n",
       "      <td>-0.496979</td>\n",
       "      <td>-0.694841</td>\n",
       "      <td>-0.219782</td>\n",
       "      <td>Eye Vertical</td>\n",
       "      <td>4.98</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>-0.612675</td>\n",
       "      <td>0.318085</td>\n",
       "      <td>-0.037831</td>\n",
       "      <td>0.722510</td>\n",
       "      <td>Head Horizontal</td>\n",
       "      <td>5.00</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>-0.387625</td>\n",
       "      <td>-0.599800</td>\n",
       "      <td>0.699429</td>\n",
       "      <td>-0.028015</td>\n",
       "      <td>Head Vertical</td>\n",
       "      <td>5.00</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>-0.504285</td>\n",
       "      <td>0.538683</td>\n",
       "      <td>0.156176</td>\n",
       "      <td>-0.656602</td>\n",
       "      <td>Eye Horizontal</td>\n",
       "      <td>5.00</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>-0.469119</td>\n",
       "      <td>-0.498881</td>\n",
       "      <td>-0.696403</td>\n",
       "      <td>-0.214636</td>\n",
       "      <td>Eye Vertical</td>\n",
       "      <td>5.00</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2004 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PC1       PC2       PC3       PC4         Features  Time  \\\n",
       "0    -0.697631  0.166558 -0.169519 -0.675894  Head Horizontal -5.00   \n",
       "1     0.710770  0.040089 -0.189767 -0.676156    Head Vertical -5.00   \n",
       "2     0.054723  0.711631 -0.642056  0.279913   Eye Horizontal -5.00   \n",
       "3    -0.071563 -0.681347 -0.723199  0.087346     Eye Vertical -5.00   \n",
       "4    -0.711701  0.098029 -0.161178 -0.676679  Head Horizontal -4.98   \n",
       "...        ...       ...       ...       ...              ...   ...   \n",
       "1999 -0.471066 -0.496979 -0.694841 -0.219782     Eye Vertical  4.98   \n",
       "2000 -0.612675  0.318085 -0.037831  0.722510  Head Horizontal  5.00   \n",
       "2001 -0.387625 -0.599800  0.699429 -0.028015    Head Vertical  5.00   \n",
       "2002 -0.504285  0.538683  0.156176 -0.656602   Eye Horizontal  5.00   \n",
       "2003 -0.469119 -0.498881 -0.696403 -0.214636     Eye Vertical  5.00   \n",
       "\n",
       "         EventName  \n",
       "0     StagEventNew  \n",
       "1     StagEventNew  \n",
       "2     StagEventNew  \n",
       "3     StagEventNew  \n",
       "4     StagEventNew  \n",
       "...            ...  \n",
       "1999  StagEventNew  \n",
       "2000  StagEventNew  \n",
       "2001  StagEventNew  \n",
       "2002  StagEventNew  \n",
       "2003  StagEventNew  \n",
       "\n",
       "[2004 rows x 7 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f8ea1ed585f811a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T16:36:37.667718Z",
     "start_time": "2025-09-02T16:36:37.664022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: components\n",
      "  - components_event_StagEventNew_timestamp_all_4_variables.csv\n",
      "Folder: cleaned_new\n",
      "  - cleaned_event_StagEventNew_timestamp_all_4_variables_.csv\n",
      "Folder: eigenvalues\n",
      "  - eigenval_event_StagEventNew_timestamp_all_4_variables.csv\n",
      "Folder: variances\n",
      "  - variances_event_StagEventNew_timestamp_all_4_variables.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets from 'data/pca_results/'\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import os\n",
    "\n",
    "base_dir = 'data/pca_results'\n",
    "\n",
    "# List all folders inside base_dir\n",
    "for folder_name in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Folder: {folder_name}\")\n",
    "        files = os.listdir(folder_path)\n",
    "        csv_files = [file_name for file_name in files if file_name.endswith('.csv')]\n",
    "        if csv_files:\n",
    "            for file_name in csv_files:\n",
    "                print(f\"  - {file_name}\")\n",
    "        else:\n",
    "            print(\"  (No .csv files found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8be8b0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CSV files in /data/pca_results/components/:\n",
      "File: components_event_StagEventNew_timestamp_all_4_variables.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>Features</th>\n",
       "      <th>Time</th>\n",
       "      <th>EventName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.697631</td>\n",
       "      <td>0.166558</td>\n",
       "      <td>-0.169519</td>\n",
       "      <td>-0.675894</td>\n",
       "      <td>Head Horizontal</td>\n",
       "      <td>-5.00</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.710770</td>\n",
       "      <td>0.040089</td>\n",
       "      <td>-0.189767</td>\n",
       "      <td>-0.676156</td>\n",
       "      <td>Head Vertical</td>\n",
       "      <td>-5.00</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.054723</td>\n",
       "      <td>0.711631</td>\n",
       "      <td>-0.642056</td>\n",
       "      <td>0.279913</td>\n",
       "      <td>Eye Horizontal</td>\n",
       "      <td>-5.00</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.071563</td>\n",
       "      <td>-0.681347</td>\n",
       "      <td>-0.723199</td>\n",
       "      <td>0.087346</td>\n",
       "      <td>Eye Vertical</td>\n",
       "      <td>-5.00</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.711701</td>\n",
       "      <td>0.098029</td>\n",
       "      <td>-0.161178</td>\n",
       "      <td>-0.676679</td>\n",
       "      <td>Head Horizontal</td>\n",
       "      <td>-4.98</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-0.471066</td>\n",
       "      <td>-0.496979</td>\n",
       "      <td>-0.694841</td>\n",
       "      <td>-0.219782</td>\n",
       "      <td>Eye Vertical</td>\n",
       "      <td>4.98</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>-0.612675</td>\n",
       "      <td>0.318085</td>\n",
       "      <td>-0.037831</td>\n",
       "      <td>0.722510</td>\n",
       "      <td>Head Horizontal</td>\n",
       "      <td>5.00</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>-0.387625</td>\n",
       "      <td>-0.599800</td>\n",
       "      <td>0.699429</td>\n",
       "      <td>-0.028015</td>\n",
       "      <td>Head Vertical</td>\n",
       "      <td>5.00</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>-0.504285</td>\n",
       "      <td>0.538683</td>\n",
       "      <td>0.156176</td>\n",
       "      <td>-0.656602</td>\n",
       "      <td>Eye Horizontal</td>\n",
       "      <td>5.00</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>-0.469119</td>\n",
       "      <td>-0.498881</td>\n",
       "      <td>-0.696403</td>\n",
       "      <td>-0.214636</td>\n",
       "      <td>Eye Vertical</td>\n",
       "      <td>5.00</td>\n",
       "      <td>StagEventNew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2004 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PC1       PC2       PC3       PC4         Features  Time  \\\n",
       "0    -0.697631  0.166558 -0.169519 -0.675894  Head Horizontal -5.00   \n",
       "1     0.710770  0.040089 -0.189767 -0.676156    Head Vertical -5.00   \n",
       "2     0.054723  0.711631 -0.642056  0.279913   Eye Horizontal -5.00   \n",
       "3    -0.071563 -0.681347 -0.723199  0.087346     Eye Vertical -5.00   \n",
       "4    -0.711701  0.098029 -0.161178 -0.676679  Head Horizontal -4.98   \n",
       "...        ...       ...       ...       ...              ...   ...   \n",
       "1999 -0.471066 -0.496979 -0.694841 -0.219782     Eye Vertical  4.98   \n",
       "2000 -0.612675  0.318085 -0.037831  0.722510  Head Horizontal  5.00   \n",
       "2001 -0.387625 -0.599800  0.699429 -0.028015    Head Vertical  5.00   \n",
       "2002 -0.504285  0.538683  0.156176 -0.656602   Eye Horizontal  5.00   \n",
       "2003 -0.469119 -0.498881 -0.696403 -0.214636     Eye Vertical  5.00   \n",
       "\n",
       "         EventName  \n",
       "0     StagEventNew  \n",
       "1     StagEventNew  \n",
       "2     StagEventNew  \n",
       "3     StagEventNew  \n",
       "4     StagEventNew  \n",
       "...            ...  \n",
       "1999  StagEventNew  \n",
       "2000  StagEventNew  \n",
       "2001  StagEventNew  \n",
       "2002  StagEventNew  \n",
       "2003  StagEventNew  \n",
       "\n",
       "[2004 rows x 7 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = 'components'\n",
    "folder_path = os.path.join(base_dir, folder)\n",
    "csv_files = glob.glob(os.path.join(folder_path, 'components_event_StagEventNew_timestamp_all_4_variables.csv'))\n",
    "print(f\"Found {len(csv_files)} CSV files in /data/pca_results/components/:\")\n",
    "for f in csv_files:\n",
    "    print(f\"File: {os.path.basename(f)}\")\n",
    "\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(\"No CSV files found in /data/pca_results/components/\")\n",
    "\n",
    "# Only allow a single file to be loaded; if multiple, warn and ask user to specify which\n",
    "if len(csv_files) == 0:\n",
    "    raise FileNotFoundError(\"No CSV files found in /data/pca_results/components/\")\n",
    "elif len(csv_files) == 1:\n",
    "    components_df = pd.read_csv(csv_files[0])\n",
    "else:\n",
    "    print(\"Warning: Multiple CSV files found in /data/pca_results/components/:\")\n",
    "    for idx, f in enumerate(csv_files):\n",
    "        print(f\"{idx}: {os.path.basename(f)}\")\n",
    "    raise RuntimeError(\"Multiple component files found. Please specify which file to load by setting the correct path or removing extra files.\")\n",
    "components_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfdea57a2643a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example color palette you provided\n",
    "color_pca = {\n",
    "    'PC1': '#F72B2B', 'PC2': '#B10606',\n",
    "    'PC3': '#962222', 'PC4': '#851E1E',\n",
    "    'PC5': '#641717', 'PC6': '#430F0F',\n",
    "    'PC7': '#3A0E0E', 'PC8': '#320C0C',\n",
    "    'PC9': '#2B0A0A', 'PC10': '#240808',\n",
    "    'PC11': '#1D0707', 'PC12': '#160505',\n",
    "    'PC13': '#100303', 'PC14': '#0A0202',\n",
    "    # 'PC15': '#050101'\n",
    "}\n",
    "\n",
    "# Select PCs to plot\n",
    "pcs = ['PC1', 'PC2']#, 'PC3']#, 'PC4']#, 'PC5']\n",
    "\n",
    "fig, ax= plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for pc in pcs:\n",
    "    ax.plot(variances_df['Time'], variances_df[pc], label=pc, color=color_pca.get(pc, '#000000'))\n",
    "# Remove the default legend as we'll create a custom one later\n",
    "ax.legend()#.remove()\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1), fontsize=12)\n",
    "sns.despine(ax=ax, offset=0, trim=False)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Variance')\n",
    "plt.title(f'Variance of Principal Components for {components_df.Features.unique()} over Time')\n",
    "# plt.legend(title='Principal Components')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f664c305c734fde1",
   "metadata": {},
   "source": [
    "# Effective dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2820844e8be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ED_estimators(eigenvalues):\n",
    "    \"\"\"\n",
    "    Compute Effective Dimensionality estimators from eigenvalues.\n",
    "    Parameters:\n",
    "    eigenvalues (list or array): List of eigenvalues.\n",
    "    Returns:\n",
    "    dict: Dictionary containing various ED estimators (n1, n2, nInf, nC).\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "\n",
    "    # Make eigenvalues a numpy array for easier manipulation\n",
    "    eigen_val = np.array(eigenvalues)\n",
    "\n",
    "    # Compute ED estimators\n",
    "    K = len(eigen_val)\n",
    "    eigen_sum = np.sum(eigen_val)\n",
    "    norm_eigen_val = eigen_val / eigen_sum\n",
    "\n",
    "    # Calculate variance using the K/(K-1) adjustment\n",
    "    eigen_var = np.var(eigen_val, ddof=0) * ((K - 1) / K)\n",
    "\n",
    "    # Calculate the four different estimators\n",
    "    output[\"n1\"] = np.prod(norm_eigen_val ** (-norm_eigen_val))\n",
    "    output[\"n2\"] = (eigen_sum ** 2) / np.sum(eigen_val ** 2)\n",
    "    output[\"nInf\"] = eigen_sum / np.max(eigen_val)\n",
    "    output[\"nC\"] = K - ((K ** 2) / (eigen_sum ** 2)) * eigen_var\n",
    "\n",
    "    return output\n",
    "\n",
    "# Process eigenvalues_df DataFrame\n",
    "def calculate_ED(df):\n",
    "    \"\"\"\n",
    "    Process eigenvalues from a DataFrame to compute ED estimators per timestamp.\n",
    "    Parameters:\n",
    "    df (DataFrame): A DataFrame containing eigenvalues and a Time column.\n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame containing ED estimators and the Time column.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Exclude Time, Event col and handle NaNs (if any)\n",
    "        eigenvalues = row[:-2].dropna()\n",
    "        time = row['Time']\n",
    "        event = row['EventName']\n",
    "        ed_estimators = compute_ED_estimators(eigenvalues)\n",
    "        ed_estimators['Time'] = time\n",
    "        ed_estimators['EventName'] = event\n",
    "        results.append(ed_estimators)\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    processed_df = pd.DataFrame(results)\n",
    "    return processed_df\n",
    "\n",
    "# Apply function to eigenvalues_df\n",
    "ed_df = calculate_ED(eigenvalues_df)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "ed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02dc5479b79776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_effective_dimensionality(ed_df, title=\"Effective Dimensionality Over Time\", features=None, save=False):\n",
    "    \"\"\"\n",
    "    Visualizes effective dimensionality measures through time.\n",
    "\n",
    "    Parameters:\n",
    "    - ed_df: DataFrame containing ED estimates with columns ['Time', 'n1', 'n2', 'nInf', 'nC'].\n",
    "    - title: Title of the plot (default: \"Effective Dimensionality Over Time\").\n",
    "    - features: List of features used in the analysis (for title display).\n",
    "    - save: Whether to save the plot as a file (default: False).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Set style\n",
    "    sns.set_theme(style=\"ticks\")\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Create a mapping for LaTeX-style labels\n",
    "    label_mapping = {\n",
    "        'n1': '$n_{1}$',\n",
    "        'n2': '$n_{2}$',\n",
    "        'nInf': '$n_{\\\\infty}$',\n",
    "        'nC': '$n_{C}$'\n",
    "    }\n",
    "    eddf = ed_df.drop(columns=['EventName'], inplace=False)\n",
    "\n",
    "    # Convert ed_df to long format for lineplot\n",
    "    ed_long = eddf.melt(id_vars=['Time'], var_name='Estimator', value_name='Value')\n",
    "\n",
    "    # Map the estimators to their LaTeX-style labels\n",
    "    ed_long['Estimator'] = ed_long['Estimator'].map(label_mapping)\n",
    "\n",
    "    # Plot all lines at once, differentiating by hue and style\n",
    "    # sns.lineplot(data=ed_long, x='Time', y='Value', hue='Estimator', style='Estimator',\n",
    "    #              palette=sns.color_palette(\"gray\", 4), linewidth=1.5, ax=ax_top)\n",
    "    sns.lineplot(data=ed_long, x='Time', y='Value', hue='Estimator', style='Estimator',\n",
    "                 palette=['#2F4858','#D83456', '#586A73', '#86888A'], linewidth=1.5, ax=ax)\n",
    "\n",
    "    # Apply styling to top plot\n",
    "    n_features = len(features) -2 # if features is not None else \"unknown number of\"\n",
    "    ax.set_title(f\"Effective dimensionality over time, {n_features} variables\", fontsize=12)\n",
    "    ax.set_ylabel(\"Effective Dimensionality\", fontsize=10)\n",
    "\n",
    "    # Hide the x-label on the top plot since we have a shared x-axis\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "    # Remove the default legend as we'll create a custom one later\n",
    "    # ax.get_legend()#.remove()\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1), fontsize=12)\n",
    "    sns.despine(ax=ax, offset=0, trim=False)\n",
    "\n",
    "    # Adjust layout to make room for the legend\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55583810e68788e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_effective_dimensionality(ed_df, features=eigenvalues_df.columns, save=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf91b23da99814",
   "metadata": {},
   "source": [
    "# PCA Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a62f88235cd42995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:48:36.626206Z",
     "start_time": "2025-06-20T11:48:36.624043Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Define the base results path\n",
    "pca_path = \"plots/\"\n",
    "\n",
    "# Create base results directory\n",
    "os.makedirs(pca_path, exist_ok=True)\n",
    "\n",
    "# Create subdirectories under the base results path\n",
    "directories = {\n",
    "    \"videos\": os.path.join(pca_path, \"videos\"),\n",
    "    # \"variances\": os.path.join(results_path, \"variances\"),\n",
    "    # \"components\": os.path.join(results_path, \"components\"),\n",
    "    # \"eigenvalues\": os.path.join(results_path, \"eigenvalues\"),\n",
    "}\n",
    "\n",
    "# Create each subdirectory\n",
    "for subdir in directories.values():\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "images_folder = r'plots/biplot/'\n",
    "video_name = \"PCA_biplot_eye_head_StagEvent_1_7fps.mp4\"\n",
    "fps = 7.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ef54f4e583286849",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:46:15.471257Z",
     "start_time": "2025-06-20T11:46:15.469421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plots/videos'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directories['videos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fac43b06e98c5700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:49:07.955960Z",
     "start_time": "2025-06-20T11:49:07.952996Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the biplots images to create a video\n",
    "def make_video(image_folder, video_name, fps):\n",
    "    images = [img for img in os.listdir(image_folder) if img.endswith(\".png\")]\n",
    "    print(images)\n",
    "    images.sort(key=lambda x: int(x.split('_')[5].split('.')[0]))\n",
    "\n",
    "    frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "\n",
    "    # Use the 'mp4v' codec\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), fps,\n",
    "                            (width, height))\n",
    "\n",
    "    for image in images:\n",
    "        video.write(cv2.imread(os.path.join(image_folder, image)))\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "55af11d8741b43c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:49:08.573407Z",
     "start_time": "2025-06-20T11:49:08.554806Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmake_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[146], line 4\u001b[0m, in \u001b[0;36mmake_video\u001b[0;34m(image_folder, video_name, fps)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_video\u001b[39m(image_folder, video_name, fps):\n\u001b[1;32m      3\u001b[0m     images \u001b[38;5;241m=\u001b[39m [img \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(image_folder) \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_folder, images[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      7\u001b[0m     height, width, layers \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[146], line 4\u001b[0m, in \u001b[0;36mmake_video.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_video\u001b[39m(image_folder, video_name, fps):\n\u001b[1;32m      3\u001b[0m     images \u001b[38;5;241m=\u001b[39m [img \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(image_folder) \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m----> 4\u001b[0m     images\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      6\u001b[0m     frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_folder, images[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      7\u001b[0m     height, width, layers \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "make_video(images_folder, video_name, fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9157199d8970145c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 PhD Main",
   "language": "python",
   "name": "phd_main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
